# Chatbot Assistant Scolaire - College Francais

## Objectif
Chatbot RAG qui repond **uniquement** a partir de cours et programmes scolaires du college francais (6eme-3eme). Le bot refuse de repondre si l'information n'existe pas dans la base de donnees.

## Current Project State
| Aspect | Status |
|--------|--------|
| Scraper Vikidia | ‚úÖ TERMINE - 8 matieres scrapees (24 321 articles, 43 857 chunks) |
| Scraper Wikiversite | ‚ùå ABANDONNE - 0.4% pages exploitables (507 pages testees, 2 lessons) |
| Scraper Academie en Ligne | ‚ùå ABANDONNE - URLs obsoletes (8 pages irrelevantes) |
| ChromaDB ingestion | ‚úÖ TERMINE - 43 870 documents ingeres dans 'cours_college' |
| Backend FastAPI | ‚úÖ PRODUCTION-READY - RAG + Auto-d√©tection + Biblioth√®que |
| Backend Auto-D√©tection | ‚úÖ TERMINE - D√©tection niveau/mati√®re par mots-cl√©s |
| Backend Biblioth√®que | ‚úÖ TERMINE - 3 nouveaux endpoints (chat/auto, lecons, detail) |
| Frontend SPA | ‚úÖ PRODUCTION-READY - 3 vues (Chat, Biblioth√®que, D√©tail) |
| Frontend Chat | ‚úÖ TERMINE - Auto-d√©tection + choix ambigu√Øt√© + history persistant |
| Frontend Biblioth√®que | ‚úÖ TERMINE - Liste le√ßons + navigation + scroll fix |
| Tests | ‚è≥ Non implementes (backend teste manuellement) |
| Deployment | ‚è≥ Local uniquement (port 8000) |
| Git status | üîÑ Nombreux fichiers modifi√©s/cr√©√©s non committ√©s |

## Last Session Summary (2026-02-07 - Session 2)
**TRANSFORMATION MAJEURE : Chatbot simple ‚Üí Plateforme d'apprentissage hybride (SPA)**

**Backend (Phase 1) - Nouvelles fonctionnalit√©s :**
1. ‚úÖ `backend/detection.py` - Auto-d√©tection niveau + mati√®re par mots-cl√©s (~180 lignes)
2. ‚úÖ Extension `backend/rag.py` - M√©thodes `get_all_lessons()` et `get_lesson_content()` (~150 lignes)
3. ‚úÖ 3 nouveaux endpoints dans `backend/main.py` :
   - `POST /api/chat/auto` - Chat avec auto-d√©tection (retourne niveau/mati√®re d√©tect√©s + ambigu√Øt√©)
   - `GET /api/lecons/{matiere}` - Liste des le√ßons d'une mati√®re (avec filtrage niveau)
   - `GET /api/lecons/{matiere}/detail?titre=...` - Contenu complet d'une le√ßon

**Frontend (Phase 2) - Refonte compl√®te en SPA :**
1. ‚úÖ Nouveau `frontend/index.html` (85 lignes) - Navigation sticky + structure SPA
2. ‚úÖ Refonte compl√®te `frontend/app.js` (784 lignes) - Router SPA + 3 vues dynamiques
3. ‚úÖ Extension `frontend/style.css` (+400 lignes, total ~1300) - Design system √©tendu

**3 Vues Impl√©ment√©es :**
- **Vue Chat** : Auto-d√©tection niveau/mati√®re, badge visible, choix si ambigu√Øt√©, history persistant
- **Vue Biblioth√®que** : Grille de le√ßons cliquables, skeleton loading, 2 boutons par le√ßon
- **Vue D√©tail Le√ßon** : Breadcrumbs, r√©sum√© + contenu complet, bouton "Poser une question"

**Bugs Corrig√©s :**
1. ‚úÖ Scroll bloqu√© dans biblioth√®que (ajout `.app-main` avec `overflow-y: auto`)
2. ‚úÖ Erreur 404 sur le√ßons (fix syntaxe filtres ChromaDB avec `$and` + `$eq`)
3. ‚úÖ Caract√®res sp√©ciaux dans URLs (passage query parameter au lieu de path)

**Design "Cahier Num√©rique" maintenu :**
- Background papier blanc (#fefdfb) avec grille 8x8px + texture SVG
- Typography: Lexend (headings) + DM Sans (body)
- 8 couleurs par mati√®re (bleu, violet, orange, vert, rose, indigo, cyan, rouge)
- Animations fluides entre vues (fadeIn, slideIn, shimmer skeletons)
- Responsive mobile-first, accessible

## Next Immediate Action

**√âTAPE 1 : Commiter tous les changements SPA**

```bash
cd C:\Users\skwar\Desktop\RAG

# Ajouter tous les fichiers cr√©√©s
git add backend/detection.py
git add CHECKPOINT.md README.md
git add docs/DEMO.md docs/FRONTEND_SUMMARY.md docs/wikiversite_scraper_guide.md

# Ajouter fichiers modifi√©s
git add backend/main.py backend/rag.py
git add frontend/index.html frontend/app.js frontend/style.css

# NE PAS AJOUTER (fichiers temporaires/config locale)
# .claude/, nul, *.log, test_*.py, data/raw/academie_en_ligne/, data/raw/wikiversite/

# Commit
git commit -m "Transform app to hybrid learning platform (SPA)

Backend (Phase 1):
- Add backend/detection.py: Auto-detect level + subject from question
- Extend backend/rag.py: Methods get_all_lessons() and get_lesson_content()
- Add 3 new endpoints: POST /api/chat/auto, GET /api/lecons/{matiere}, GET /api/lecons/{matiere}/detail

Frontend (Phase 2):
- Complete SPA refactor: Router + 3 dynamic views (Chat, Library, Lesson Detail)
- Vue Chat: Auto-detection with visible badge, ambiguity choice, persistent history
- Vue Library: Clickable lessons grid, skeleton loading, filters
- Vue Detail: Breadcrumbs, summary + full content, ask question button

Fixes:
- Fix scroll blocked in library (add .app-main overflow)
- Fix 404 on lessons (fix ChromaDB filters syntax with \$and + \$eq)
- Fix special chars in URLs (use query param instead of path)

Frontend extensions:
- frontend/index.html: 85 lines (sticky nav + SPA structure)
- frontend/app.js: 784 lines (complete rewrite)
- frontend/style.css: +400 lines (extended design system)

Status: App fully functional, 43,870 Vikidia lessons browsable
Next: Test all features then push to GitHub

Co-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>"

# Push vers GitHub
git push origin main
```

**√âTAPE 2 : Tester toutes les fonctionnalit√©s**

1. Lancer le backend :
   ```bash
   cd backend
   uvicorn main:app --reload --port 8000
   ```

2. Ouvrir http://localhost:8000

3. **Tester Vue Chat :**
   - Question : "C'est quoi le th√©or√®me de Pythagore ?"
   - V√©rifier badge "ü§ñ D√©tect√© : 5√®me ‚Ä¢ Math√©matiques"
   - Question ambigu√´ : "Parle-moi de la r√©volution" ‚Üí V√©rifier boutons de choix

4. **Tester Vue Biblioth√®que :**
   - Cliquer sur "üìö Biblioth√®que"
   - Cliquer sur "üìê Maths"
   - V√©rifier liste des le√ßons (doit afficher ~543 le√ßons)
   - V√©rifier scroll fonctionne

5. **Tester Vue D√©tail :**
   - Cliquer "üìñ Lire" sur une le√ßon (ex: "Th√©or√®me de Pythagore")
   - V√©rifier breadcrumbs cliquables
   - Cliquer "üìñ Lire le contenu complet"
   - Cliquer "üí¨ Poser une question" ‚Üí Retour au chat avec question pr√©-remplie

6. **Tester Navigation :**
   - V√©rifier historique chat conserv√© quand on change de vue
   - V√©rifier bouton retour navigateur fonctionne
   - V√©rifier th√®me couleur change selon mati√®re s√©lectionn√©e

**Prochaines √©tapes possibles :**
- Ajouter tests automatis√©s (pytest backend, playwright frontend)
- D√©ployer sur Render/Railway (n√©cessite: requirements.txt complet, Procfile)
- Ajouter fonctionnalit√©s : export PDF, mode sombre, voice input
- Am√©liorer d√©tection auto (ML model au lieu de mots-cl√©s)

## Stack technique
- **Backend** : Python 3.11+ / FastAPI / LangChain
- **Vector DB** : ChromaDB (persistance locale dans `chromadb/`)
- **LLM** : OpenAI GPT-4o-mini
- **Embeddings** : OpenAI text-embedding-3-small
- **Frontend** : HTML / CSS / JS vanilla
- **Scraping** : cloudscraper + BeautifulSoup + API MediaWiki (Cloudflare bypass)

## Structure du projet
```
RAG/
‚îú‚îÄ‚îÄ scraper/        # Collecte et traitement des donnees scolaires
‚îÇ   ‚îú‚îÄ‚îÄ vikidia.py      # Scraper Vikidia (API MediaWiki + cloudscraper)
‚îÇ   ‚îú‚îÄ‚îÄ cleaner.py      # Nettoyage texte (LaTeX, HTML, sections inutiles)
‚îÇ   ‚îú‚îÄ‚îÄ chunker.py      # Decoupage en chunks ~500 tokens avec overlap
‚îÇ   ‚îú‚îÄ‚îÄ metadata.py     # Categories, matieres, niveaux
‚îÇ   ‚îî‚îÄ‚îÄ pipeline.py     # Orchestration scrape -> clean -> chunk -> save
‚îú‚îÄ‚îÄ backend/        # API FastAPI + chaine RAG LangChain
‚îú‚îÄ‚îÄ frontend/       # Interface chat HTML/CSS/JS
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/vikidia/    # Articles bruts par matiere (JSON)
‚îÇ   ‚îî‚îÄ‚îÄ processed/      # Chunks prets pour embedding (JSON)
‚îú‚îÄ‚îÄ chromadb/       # Base vectorielle ChromaDB (persistee)
‚îú‚îÄ‚îÄ docs/plans/     # Plans d'implementation
‚îú‚îÄ‚îÄ .env            # Cles API (NE PAS COMMITTER)
‚îî‚îÄ‚îÄ requirements.txt
```

## Sources de donnees
- **Vikidia** : articles encyclopediques adaptes aux collegiens (API MediaWiki) - ‚úÖ UTILISE (43 857 chunks)
- **Wikiversite** : cours structures niveau college (API MediaWiki) - ‚ùå ABANDONNE (99.6% pages vides)
- **Academie en Ligne (CNED)** : cours par niveau - ‚ùå ABANDONNE (site restructure, URLs obsoletes)

## Strategie d'adaptation par niveau
Comme les sources alternatives (Wikiversite, Academie en Ligne) n'ont pas de contenu exploitable, l'adaptation par niveau se fait via **prompts GPT personnalises** :
- Tous les chunks Vikidia sont tagges `niveau: "college"` (generique)
- Le backend utilise des prompts adaptes selon le niveau de l'eleve (6eme, 5eme, 4eme, 3eme)
- GPT-4o-mini adapte le langage et les explications au niveau specifie dans la requete

## Matieres
| Matiere | Source Vikidia | Articles | Chunks |
|---------|---------------|----------|--------|
| Histoire-Geo | Categories:Histoire,Geographie | 13 112 | 25 474 |
| SVT | Categories:Biologie,Geologie | 5 454 | 8 481 |
| Francais | Categories:Francais,Grammaire,Litterature | 3 040 | 4 835 |
| Physique-Chimie | Categories:Physique,Chimie | 1 439 | 2 751 |
| Technologie | Categories:Technologie | 724 | 1 349 |
| Mathematiques | Categories:Mathematiques | 543 | 967 |
| Anglais | Categories:Anglais | 6 | - |
| Espagnol | Categories:Espagnol | 3 | - |
| **TOTAL** | | **24 321** | **43 857** |

## Niveaux
- 6eme (cycle 3)
- 5eme, 4eme, 3eme (cycle 4)

## Contraintes critiques
1. **Reponse uniquement depuis la base** : le LLM ne doit JAMAIS utiliser ses connaissances propres
2. **Filtrage niveau/matiere** : chaque chunk est tague avec metadonnees
3. **Refus hors-perimetre** : si aucun chunk pertinent trouve, le bot dit qu'il ne sait pas
4. **Pas de secrets dans le code** : utiliser `.env` pour les cles API

## Commandes utiles
```bash
# Installer les dependances
pip install -r requirements.txt

# Lancer le scraping (toutes matieres)
python -m scraper.pipeline

# Lancer le scraping (une matiere)
python -m scraper.pipeline --matiere mathematiques

# Lancer le backend
uvicorn backend.main:app --reload
```

## Notes techniques
- Vikidia est protege par Cloudflare -> utiliser `cloudscraper` (pas `requests` direct)
- Delai de 1s entre chaque requete API pour respecter le serveur
- Crawl recursif des sous-categories (profondeur max 3)
- Deduplication des articles vus dans plusieurs categories
- Logs avec indicateur [PROGRESSION] toutes les 10 secondes
